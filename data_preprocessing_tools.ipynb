{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                    # work with arrays\n",
    "import pandas as pd                   # import datasets\n",
    "import matplotlib.pyplot as plt       # Plot chart "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "x = dataset.iloc[:, :-1].values          # independent columns (features)\n",
    "y = dataset.iloc[:, -1].values          # dependent column (predicting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)                           # print x contents\n",
    "print(\"=================================\")\n",
    "print(y)                           # print y contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking Care of Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer               # to replace the missing values in the datsets with avg,min,max etc \n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = \"mean\")       # object of simple imputer\n",
    "imputer.fit(x[:, 1:])                                  # only columns with numerical missing data\n",
    "x[:, 1:] = imputer.transform(x[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)        # check for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strings / categorical data is difficult to compute by ML\n",
    "from sklearn.compose import ColumnTransformer               # to transform the country column into One Hot Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [0])], remainder = 'passthrough')   # passthrough is used to keep the other columns of x\n",
    "X_new = np.array(ct.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder          # Encode into binary values\n",
    "encoder = LabelEncoder()\n",
    "y_new = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to do feature scaling after splitting\n",
    "# traing : test = 8 : 2\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_new, y_new, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)\n",
    "print(\"\\n\")\n",
    "print(X_test)\n",
    "print(\"\\n\")\n",
    "print(Y_train)\n",
    "print(\"\\n\")\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to put all features to same scale\n",
    "# two main feature scaling techniques \n",
    "# standardisation -> all the time, normalisation -> normal distribution\n",
    "# x(std) = (x - mean(x))/(std(x))               [-3, 3]\n",
    "# x(norm) = (x - min(x))/ (max(x) - min(x))     [0, 1]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stsc = StandardScaler()\n",
    "\n",
    "# we do not apply feature scalling on dummy variables\n",
    "X_train[:, 3:] = stsc.fit_transform(X_train[:, 3:])\n",
    "X_test[:, 3:] = stsc.transform(X_test[:, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)\n",
    "print(\"\\n\")\n",
    "print(X_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocessing_tools.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
